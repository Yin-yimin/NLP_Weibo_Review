{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baidu_API表格处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('data.xlsx')\n",
    "baidu = pd.read_csv('output/df_baidu.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6694 6694\n"
     ]
    }
   ],
   "source": [
    "print(len(df),len(baidu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5554\n"
     ]
    }
   ],
   "source": [
    "merged_df = pd.merge(df, baidu, on='texts', how='left')\n",
    "merged_df.drop_duplicates(subset=['texts'], inplace=True)   # 去重\n",
    "print(len(merged_df))\n",
    "merged_df.to_excel('output/df_baidu_merged.xlsx', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "去重以后长度有点不一样，原因可能是调用api的时候有部分文本有问题or空白，没有分析出情感倾向"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## emotion_score的处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>words</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>徐州之前刚因为铁链女出大风头，好不容易大家忘了点。结果又开始蹭徐州才是淄博烧烤发源地热度，b...</td>\n",
       "      <td>['徐州', '之前', '刚', '铁链', '女出', '风头', '好不容易', '忘...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>不好意思对徐州的印象只有八孩铁链女和拐卖妇女</td>\n",
       "      <td>['不好意思', '徐州', '印象', '八孩', '铁链', '女', '拐卖妇女', ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>徐州只能让我想起一件事情，铁链女🙈🙉🙊</td>\n",
       "      <td>['徐州', '只能', '想起', '一件', '事情', '铁链', '女', '🙈',...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>铁链女，丰县八孩</td>\n",
       "      <td>['铁链', '女', '丰县', '八孩', ' ']</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>想知道这个女的后续</td>\n",
       "      <td>['想', '知道', '女', '后续', ' ']</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6689</th>\n",
       "      <td>强奸犯不是扬某侠的家人，强奸生下的不是“人”是被强奸的恶果</td>\n",
       "      <td>['强奸犯', '扬', '某侠', '家人', '强奸', '生下', '强奸', '恶果']</td>\n",
       "      <td>-18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6690</th>\n",
       "      <td>如果这事调查不力，让人贩子兼强奸犯逍遥法外，不能保护好“她”，只能说明我们中国政府大力支持人...</td>\n",
       "      <td>['事', '调查', '不力', '人贩子', '兼', '强奸犯', '逍遥法外', '...</td>\n",
       "      <td>-17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6691</th>\n",
       "      <td>越烂的基因越是拼命想着要延续下去。</td>\n",
       "      <td>['越烂', '基因', '拼命', '想着', '延续下去']</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6692</th>\n",
       "      <td>怎么我们女生好欺负吗？就这样谁敢出门，男女平等？？女生在社会上是弱势群体，不应该得到政府，男...</td>\n",
       "      <td>['怎么', '女生', '欺负', '敢', '出门', '男女平等', '女生', '社...</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6693</th>\n",
       "      <td>生前遗物怎么进行DNA比对？ 家长让14岁的桑某妞领着18岁的小花梅找个好人家，然后14岁的...</td>\n",
       "      <td>['生前', '遗物', '怎么', '进行', 'DNA', ' ', '家长', '14...</td>\n",
       "      <td>-13.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6694 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  texts  \\\n",
       "0     徐州之前刚因为铁链女出大风头，好不容易大家忘了点。结果又开始蹭徐州才是淄博烧烤发源地热度，b...   \n",
       "1                              不好意思对徐州的印象只有八孩铁链女和拐卖妇女     \n",
       "2                                  徐州只能让我想起一件事情，铁链女🙈🙉🙊    \n",
       "3                                             铁链女，丰县八孩    \n",
       "4                                            想知道这个女的后续    \n",
       "...                                                 ...   \n",
       "6689                      强奸犯不是扬某侠的家人，强奸生下的不是“人”是被强奸的恶果   \n",
       "6690  如果这事调查不力，让人贩子兼强奸犯逍遥法外，不能保护好“她”，只能说明我们中国政府大力支持人...   \n",
       "6691                                  越烂的基因越是拼命想着要延续下去。   \n",
       "6692  怎么我们女生好欺负吗？就这样谁敢出门，男女平等？？女生在社会上是弱势群体，不应该得到政府，男...   \n",
       "6693  生前遗物怎么进行DNA比对？ 家长让14岁的桑某妞领着18岁的小花梅找个好人家，然后14岁的...   \n",
       "\n",
       "                                                  words  score  \n",
       "0     ['徐州', '之前', '刚', '铁链', '女出', '风头', '好不容易', '忘...    0.0  \n",
       "1     ['不好意思', '徐州', '印象', '八孩', '铁链', '女', '拐卖妇女', ...    0.0  \n",
       "2     ['徐州', '只能', '想起', '一件', '事情', '铁链', '女', '🙈',...    0.0  \n",
       "3                          ['铁链', '女', '丰县', '八孩', ' ']    0.0  \n",
       "4                           ['想', '知道', '女', '后续', ' ']    0.0  \n",
       "...                                                 ...    ...  \n",
       "6689   ['强奸犯', '扬', '某侠', '家人', '强奸', '生下', '强奸', '恶果']  -18.0  \n",
       "6690  ['事', '调查', '不力', '人贩子', '兼', '强奸犯', '逍遥法外', '...  -17.0  \n",
       "6691                   ['越烂', '基因', '拼命', '想着', '延续下去']    0.0  \n",
       "6692  ['怎么', '女生', '欺负', '敢', '出门', '男女平等', '女生', '社...   -3.0  \n",
       "6693  ['生前', '遗物', '怎么', '进行', 'DNA', ' ', '家长', '14...  -13.0  \n",
       "\n",
       "[6694 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_score = pd.read_csv('output/emotion_score.csv')\n",
    "emotion_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_emo = pd.merge(df, emotion_score, on='texts', how='left')\n",
    "merged_df_emo.to_excel('output/df_emo_score_merged.xlsx', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 合并所有数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cemotion\n",
    "df1 = pd.read_excel('output/data_cemotion.xlsx')\n",
    "df1 = df1.rename(columns={'sentiment_polar': 'cemotion_senti'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6694"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge baidu\n",
    "df2 = pd.read_excel('output/df_baidu_merged.xlsx')\n",
    "df2 = df2[['texts','pm','pp','np']]\n",
    "df2.columns = ['texts','baidu_pm','baidu_pp','baidu_np']\n",
    "df2 = pd.merge(df1, df2, on='texts', how='left')\n",
    "len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6694"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge emo classification\n",
    "df3 = pd.read_excel('output/data_emo_classify.xlsx')\n",
    "df3 = df3[['texts','length', 'positive', 'negative', 'anger', 'disgust', 'fear', 'sadness', 'surprise', 'good', 'happy']]\n",
    "len(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "[1497, 1504, 1708, 1709, 1841, 2101, 2221, 2732, 2734, 2835, 2841, 3054, 3186, 3279, 3373, 3474, 3546, 3615, 3629, 3632, 3662, 3689, 3982, 4044, 4068, 4070, 4073, 4082, 4083, 4084, 4086, 4087, 4088, 4089, 4091, 4094, 4095, 4097, 4119, 4120, 4205, 4209, 4259, 4261, 4283, 4351, 4404, 4432, 4675, 4717, 4734, 4747, 4748, 4750, 4751, 4753, 4755, 4756, 4808, 4886, 4892, 4901, 5000, 5054, 5059, 5073, 5111, 5121, 5122, 5135, 5152, 5184, 5185, 5186, 5187, 5258, 5259, 5260, 5502, 5505, 5644, 5675, 5684, 5688, 5770, 5794, 5844, 5858]\n"
     ]
    }
   ],
   "source": [
    "# 直接concat不知道为什么出现了问题，检测一下\n",
    "fail = []\n",
    "for i in range(len(df2)):\n",
    "    if df2['texts'][i] != df3['texts'][i]:\n",
    "        print(df2['texts'][i], df3['texts'][i])\n",
    "        fail.append(i)\n",
    "print(fail)\n",
    "# index都是一致的，出问题的是因为nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Unnamed: 0.1', 'comments_count', 'attitudes_count',\n",
       "       'reposts_count', 'created_at', 'status_city', 'status_country',\n",
       "       'status_province', 'user_id', 'id', 'isLongText', 'user_screen_name',\n",
       "       'user_verified', 'user_gender', 'user_follow_count',\n",
       "       'user_followers_count', 'user_verified_reason', 'weibo_emoji', 'tags',\n",
       "       'texts', 'cemotion_senti', 'baidu_pm', 'baidu_pp', 'baidu_np'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['texts', 'length', 'positive', 'negative', 'anger', 'disgust', 'fear',\n",
       "       'sadness', 'surprise', 'good', 'happy'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments_count</th>\n",
       "      <th>attitudes_count</th>\n",
       "      <th>reposts_count</th>\n",
       "      <th>created_at</th>\n",
       "      <th>status_city</th>\n",
       "      <th>status_country</th>\n",
       "      <th>status_province</th>\n",
       "      <th>user_id</th>\n",
       "      <th>id</th>\n",
       "      <th>isLongText</th>\n",
       "      <th>...</th>\n",
       "      <th>length</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>anger</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>good</th>\n",
       "      <th>happy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-04-25 13:48:34</td>\n",
       "      <td>济南</td>\n",
       "      <td>中国</td>\n",
       "      <td>山东</td>\n",
       "      <td>5953395370</td>\n",
       "      <td>4894409889415605</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-04-25 13:48:33</td>\n",
       "      <td>信阳</td>\n",
       "      <td>中国</td>\n",
       "      <td>河南</td>\n",
       "      <td>6046338053</td>\n",
       "      <td>4894409886011988</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-04-25 13:48:01</td>\n",
       "      <td>中山</td>\n",
       "      <td>中国</td>\n",
       "      <td>广东</td>\n",
       "      <td>6270833313</td>\n",
       "      <td>4894409751270799</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-04-25 13:44:46</td>\n",
       "      <td>北京</td>\n",
       "      <td>中国</td>\n",
       "      <td>北京</td>\n",
       "      <td>5334170262</td>\n",
       "      <td>4894408934426148</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-04-25 13:40:10</td>\n",
       "      <td>武汉</td>\n",
       "      <td>中国</td>\n",
       "      <td>湖北</td>\n",
       "      <td>6056043508</td>\n",
       "      <td>4894407776799644</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6689</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-02-12 13:23:55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6912848963</td>\n",
       "      <td>4736040188775897</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6690</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-02-12 03:42:37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3087874701</td>\n",
       "      <td>4735893900362458</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6691</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-02-12 01:21:25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1985123417</td>\n",
       "      <td>4735858370150463</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6692</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-02-12 01:16:59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6628549450</td>\n",
       "      <td>4735857250010046</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6693</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-02-12 01:11:15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2460468142</td>\n",
       "      <td>4735855807432484</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6694 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      comments_count  attitudes_count  reposts_count          created_at  \\\n",
       "0                  0               41              0 2023-04-25 13:48:34   \n",
       "1                  1                8              0 2023-04-25 13:48:33   \n",
       "2                  0               20              0 2023-04-25 13:48:01   \n",
       "3                  1                2              0 2023-04-25 13:44:46   \n",
       "4                  0                2              0 2023-04-25 13:40:10   \n",
       "...              ...              ...            ...                 ...   \n",
       "6689               0                1              0 2022-02-12 13:23:55   \n",
       "6690               0                0              0 2022-02-12 03:42:37   \n",
       "6691               0                1              0 2022-02-12 01:21:25   \n",
       "6692               0                1              0 2022-02-12 01:16:59   \n",
       "6693               0                0              0 2022-02-12 01:11:15   \n",
       "\n",
       "     status_city status_country status_province     user_id                id  \\\n",
       "0             济南             中国              山东  5953395370  4894409889415605   \n",
       "1             信阳             中国              河南  6046338053  4894409886011988   \n",
       "2             中山             中国              广东  6270833313  4894409751270799   \n",
       "3             北京             中国              北京  5334170262  4894408934426148   \n",
       "4             武汉             中国              湖北  6056043508  4894407776799644   \n",
       "...          ...            ...             ...         ...               ...   \n",
       "6689         NaN            NaN             NaN  6912848963  4736040188775897   \n",
       "6690         NaN            NaN             NaN  3087874701  4735893900362458   \n",
       "6691         NaN            NaN             NaN  1985123417  4735858370150463   \n",
       "6692         NaN            NaN             NaN  6628549450  4735857250010046   \n",
       "6693         NaN            NaN             NaN  2460468142  4735855807432484   \n",
       "\n",
       "      isLongText  ... length  positive negative  anger  disgust fear sadness  \\\n",
       "0          False  ...     37         1        1      0        1    0       0   \n",
       "1          False  ...      9         0        1      0        0    1       0   \n",
       "2          False  ...     11         0        0      0        0    0       0   \n",
       "3          False  ...      5         0        0      0        0    0       0   \n",
       "4          False  ...      5         0        0      0        0    0       0   \n",
       "...          ...  ...    ...       ...      ...    ...      ...  ...     ...   \n",
       "6689       False  ...      8         0        2      0        2    0       0   \n",
       "6690       False  ...     24         0        3      0        3    0       0   \n",
       "6691       False  ...      5         1        0      0        0    0       0   \n",
       "6692       False  ...     19         1        2      0        2    0       0   \n",
       "6693       False  ...     60         0        2      0        1    0       1   \n",
       "\n",
       "     surprise good  happy  \n",
       "0           0    1      0  \n",
       "1           0    0      0  \n",
       "2           0    0      0  \n",
       "3           0    0      0  \n",
       "4           0    0      0  \n",
       "...       ...  ...    ...  \n",
       "6689        0    0      0  \n",
       "6690        0    0      0  \n",
       "6691        0    1      0  \n",
       "6692        0    1      0  \n",
       "6693        0    0      0  \n",
       "\n",
       "[6694 rows x 33 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df4 = pd.concat([df2, df3[['length', 'positive', 'negative', 'anger', 'disgust', 'fear',\n",
    "       'sadness', 'surprise', 'good', 'happy']]], axis=1, ignore_index=True)\n",
    "merged_df4.columns = ['Unnamed: 0', 'Unnamed: 0.1', 'comments_count', 'attitudes_count',\n",
    "       'reposts_count', 'created_at', 'status_city', 'status_country',\n",
    "       'status_province', 'user_id', 'id', 'isLongText', 'user_screen_name',\n",
    "       'user_verified', 'user_gender', 'user_follow_count',\n",
    "       'user_followers_count', 'user_verified_reason', 'weibo_emoji', 'tags',\n",
    "       'texts', 'cemotion_senti', 'baidu_pm', 'baidu_pp', 'baidu_np', 'length', 'positive', 'negative', 'anger', 'disgust', 'fear',\n",
    "       'sadness', 'surprise', 'good', 'happy']\n",
    "merged_df4 = merged_df4.drop(merged_df4.columns[0], axis=1)\n",
    "merged_df4 = merged_df4.drop(merged_df4.columns[0], axis=1)\n",
    "\n",
    "merged_df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6694\n",
      "Index(['Unnamed: 0', 'comments_count', 'attitudes_count', 'reposts_count',\n",
      "       'created_at', 'status_city', 'status_country', 'status_province',\n",
      "       'user_id', 'id', 'isLongText', 'user_screen_name', 'user_verified',\n",
      "       'user_gender', 'user_follow_count', 'user_followers_count',\n",
      "       'user_verified_reason', 'weibo_emoji', 'tags', 'texts', 'words',\n",
      "       'score'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# merge df_emo_score_merged.xlsx\n",
    "df5 = pd.read_excel('output/df_emo_score_merged.xlsx')\n",
    "df5 = df5.drop_duplicates()\n",
    "print(len(df5))\n",
    "print(df5.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.to_excel('output/df_emo_score_merged.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['comments_count', 'attitudes_count', 'reposts_count', 'created_at',\n",
       "       'status_city', 'status_country', 'status_province', 'user_id', 'id',\n",
       "       'isLongText', 'user_screen_name', 'user_verified', 'user_gender',\n",
       "       'user_follow_count', 'user_followers_count', 'user_verified_reason',\n",
       "       'weibo_emoji', 'tags', 'texts', 'cemotion_senti', 'baidu_pm',\n",
       "       'baidu_pp', 'baidu_np', 'length', 'positive', 'negative', 'anger',\n",
       "       'disgust', 'fear', 'sadness', 'surprise', 'good', 'happy'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df4.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df5[['texts','words','score']]\n",
    "merged_df4 = merged_df4.reset_index(drop=True)\n",
    "df5 = df5.reset_index(drop=True)\n",
    "\n",
    "df_all = pd.concat([merged_df4, df5[['words','score']]], axis=1, ignore_index=True)\n",
    "df_all.columns = ['comments_count', 'attitudes_count', 'reposts_count', 'created_at',\n",
    "       'status_city', 'status_country', 'status_province', 'user_id', 'id',\n",
    "       'isLongText', 'user_screen_name', 'user_verified', 'user_gender',\n",
    "       'user_follow_count', 'user_followers_count', 'user_verified_reason',\n",
    "       'weibo_emoji', 'tags', 'texts', 'cemotion_senti', 'baidu_pm',\n",
    "       'baidu_pp', 'baidu_np', 'length', 'positive', 'negative', 'anger',\n",
    "       'disgust', 'fear', 'sadness', 'surprise', 'good', 'happy','words','emo_score']\n",
    "df_all.to_excel('data_process_all.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6694 6694 6694\n"
     ]
    }
   ],
   "source": [
    "print(len(merged_df4),len(df5),len(df_all))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
